# Chatbot RAG para consultar el CV del alumno (Streamlit)

Sistema de **Retrieval-Augmented Generation (RAG)** que permite consultar un CV y obtener respuestas con citas de las secciones relevantes.
Listo para ejecutar localmente con **Streamlit + FAISS + Sentence-Transformers** y generaciÃ³n vÃ­a **Ollama** (local) u **OpenAI** (SaaS).

## ğŸš€ Demo rÃ¡pida (5 pasos)

```bash
# 1) Clonar o copiar esta carpeta
cd rag_cv_chatbot

# 2) Crear entorno e instalar deps
python -m venv .venv
# Windows
.venv\Scripts\activate
# Mac/Linux
source .venv/bin/activate
pip install -r requirements.txt

# 3) (Opcional) Configurar modelo de generaciÃ³n
#    OpciÃ³n A - Local con Ollama (recomendado, gratis):
#      - Instalar https://ollama.com/download
#      - Descargar modelo:  ollama pull llama3.1:8b
#      - Exportar variable:  set OLLAMA_MODEL=llama3.1:8b   (Windows)
#                            export OLLAMA_MODEL=llama3.1:8b (Mac/Linux)
#
#    OpciÃ³n B - OpenAI:
#      - Copiar .env.example a .env y setear OPENAI_API_KEY=<tu_api_key>

# 4) Colocar tu CV en /data como PDF/TXT/MD (ej: cv_alumno.pdf).
#    TambiÃ©n podÃ©s subirlo desde la UI.
python ingest.py  # crea/actualiza el Ã­ndice

# 5) Lanzar la app
streamlit run app.py
```

Abre el navegador en la URL que te imprime Streamlit (usualmente `http://localhost:8501`).

---

## ğŸ“ Estructura

```
rag_cv_chatbot/
â”œâ”€ app.py                # UI Streamlit (chat + upload + reindex)
â”œâ”€ ingest.py             # Ingesta de documentos de /data -> Ã­ndice FAISS
â”œâ”€ rag.py                # NÃºcleo: embedder, retriever y generaciÃ³n
â”œâ”€ utils.py              # Carga y troceo de documentos
â”œâ”€ requirements.txt
â”œâ”€ .env.example
â”œâ”€ data/
â”‚  â””â”€ CV_ejemplo.md
â”œâ”€ storage/              # Se crea al ejecutar ingest.py (Ã­ndice y metadatos)
â””â”€ docs/
   â””â”€ demo_script.md     # Guion sugerido para tu video de presentaciÃ³n
```

## âœ¨ CaracterÃ­sticas

- **Ingesta simple**: arrastrÃ¡ tu CV (PDF/TXT/MD) o reemplazÃ¡ el ejemplo y ejecutÃ¡ `python ingest.py`.
- **Embeddings multilingÃ¼es**: `paraphrase-multilingual-MiniLM-L12-v2` (soporta espaÃ±ol).
- **FAISS** como vector store en disco.
- **RAG** con *top-k retrieval* y citas (fragmentos + archivo origen).
- **Modelo de texto** intercambiable: **Ollama local** o **OpenAI** (selecciÃ³n automÃ¡tica por variables de entorno).
- **Streamlit** con historial de chat y subida de archivos.

## ğŸ§  Â¿CÃ³mo funciona?

1. **Troceo del CV** en fragmentos (~500 tokens con solapamiento).
2. **Embeddings** de cada chunk y construcciÃ³n del Ã­ndice **FAISS**.
3. En consulta, se recuperan los **k** fragmentos mÃ¡s similares.
4. Se genera una respuesta con un **prompt con contexto** que incluye los fragmentos recuperados.
5. Se muestran **citas** (archivo y preview del texto) para trazabilidad.

## âš™ï¸ ConfiguraciÃ³n

- Variables de entorno (en `.env` si usÃ¡s OpenAI):
  - `OPENAI_API_KEY`: clave para usar modelos de OpenAI.
  - `OPENAI_MODEL` (opcional): por defecto `gpt-4o-mini`.
  - `OLLAMA_MODEL` (opcional): p.ej. `llama3.1:8b` si usÃ¡s Ollama local.

- ParÃ¡metros principales (editables en `rag.py`):
  - `EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"`
  - `CHUNK_SIZE = 1000` (caracteres aproximados)
  - `CHUNK_OVERLAP = 150`
  - `TOP_K = 4`

## ğŸ§ª Pruebas rÃ¡pidas de preguntas

- *"Â¿CuÃ¡l es la formaciÃ³n acadÃ©mica del alumno?"*
- *"Enumera experiencias relevantes en QA/IA con fechas."*
- *"Â¿QuÃ© habilidades tÃ©cnicas tiene y en quÃ© proyectos las aplicÃ³?"*
- *"Â¿Datos de contacto?"*

## ğŸ¥ Entregable del video (OBS)

1. Muestra el repo y `README.md`.
2. EjecutÃ¡ `python ingest.py` con tu **CV real** en `/data` o subilo desde la UI.
3. AbrÃ­ `streamlit run app.py`, hacÃ© 2â€“3 consultas y mostrÃ¡ citas.
4. FinalizÃ¡ explicando brevemente la arquitectura RAG.

## ğŸ§¾ Licencia

MIT. PodÃ©s reutilizar y modificar libremente citando este repo en tu TP.


---

## ğŸ§‘â€ğŸ’¼ Nuevo: Modo **Multiâ€‘Agentes por Persona**

Este repo ahora soporta **1 agente por persona**. Cada agente tiene su **propio Ã­ndice RAG** (FAISS) limitado a los documentos de su carpeta en `data/<Persona>/`.  
**Reglas clave:**

- Si la consulta **no nombra a nadie**, se usa el **agente del Alumno** (configurable en `config/people.json` â†’ `default_student`).
- Si la consulta **nombra a mÃ¡s de una persona**, se **ejecutan mÃºltiples agentes** y se compone una respuesta con **secciones por persona** y **citas**.
- Estructura esperada de datos:
  ```text
  data/
    Alumno/      # CVs del alumno (por defecto)
    Persona2/
    Persona3/
  ```

### â–¶ï¸ Pasos rÃ¡pidos

```bash
# 0) (opcional) Crear venv + instalar deps
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# 1) Editar personas y alias
cp -n config/people.json config/people.json  # ya existe con ejemplo
# AbrÃ­ config/people.json y ajustÃ¡ nombres/alias y default_student

# 2) Colocar CVs (PDF/TXT/MD) en /data/<Persona>/
mkdir -p data/Alumno
cp data/CV_ejemplo.md data/Alumno/  # ejemplo

# 3) Construir Ã­ndices por persona
python ingest_agents.py

# 4) Ejecutar la app multiâ€‘agentes (Streamlit)
streamlit run app_agents.py
```

> **Nota de evaluaciÃ³n:** El video debe mostrar: consulta por defecto (sin nombre â†’ Alumno), consulta a otra persona nombrada, y consulta a **2 personas a la vez** con respuestas en secciones y citas por persona.

### ğŸ¥ Guion sugerido para el video (â‰¤ 2 min)

1. **Intro (10s):** objetivo del TP y que hay *1 agente por persona*.
2. **Arquitectura (20s):** `data/<Persona> â†’ ingest_agents.py â†’ storage/<Persona> (FAISS) â†’ app_agents.py`.
3. **Demo (60s):**
   - Pregunta sin nombre (â€œÂ¿CuÃ¡l es tu formaciÃ³n?â€) â†’ responde Alumno.
   - Pregunta â€œÂ¿QuÃ© experiencia tienen *Alumno* y *Persona2* en Python?â€ â†’ muestra secciones por persona + citas.
4. **Cierre (20s):** cÃ³mo agregar una tercera persona y repetir los pasos.

